{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42df02ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Batch-optimized setup complete!\n",
      "üìù Run 'quick_batch_test()' to test batch processing.\n",
      "üìÅ Use 'process_directory()' for real datasets.\n",
      "üíæ Use 'StreamingDepthProcessor' for very large datasets.\n",
      "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /Users/adamaslan/.cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_hybrid_384.pt\" to /Users/adamaslan/.cache/torch/hub/checkpoints/dpt_hybrid_384.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 470M/470M [00:32<00:00, 15.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Batch Pipeline initialized:\n",
      "   Model: midas_small\n",
      "   Input size: 256x256\n",
      "   Device: mps\n",
      "   Max batch size: 4\n",
      "   Workers: 8\n",
      "üì¶ Processing batch 1/1\n",
      "   ‚úÖ Batch completed in 7.44s (4 images)\n",
      "\n",
      "üéØ Batch processing complete:\n",
      "   Total images: 4\n",
      "   Successfully processed: 4\n",
      "   Total time: 7.44s\n",
      "   Average per image: 1.86s\n",
      "{'image_path': 'public/numbers-stonks1.png', 'analysis': {'type': '3d_object', 'confidence': 0.6}, 'points': array([[-0.19432224, -0.19432224,  0.38864449],\n",
      "       [-0.22272596, -0.22991067,  0.45982134],\n",
      "       [-0.26137806, -0.27880326,  0.55760652],\n",
      "       ...,\n",
      "       [ 4.3046149 ,  4.60148489,  9.49983978],\n",
      "       [ 4.48104948,  4.63041779,  9.55957222],\n",
      "       [ 4.67940703,  4.67940703,  9.66071129]], shape=(4096, 3)), 'colors': array([[1.        , 1.        , 1.        ],\n",
      "       [1.        , 1.        , 1.        ],\n",
      "       [1.        , 1.        , 1.        ],\n",
      "       ...,\n",
      "       [0.9372549 , 0.9372549 , 0.9372549 ],\n",
      "       [0.93333333, 0.93333333, 0.93333333],\n",
      "       [0.93333333, 0.93333333, 0.93333333]], shape=(4096, 3)), 'depth_map': array([[0.03886445, 0.03218777, 0.04029476, ..., 0.5361968 , 0.5220597 ,\n",
      "        0.13887186],\n",
      "       [0.        , 0.0315074 , 0.03905117, ..., 0.5352243 , 0.51761484,\n",
      "        0.52549136],\n",
      "       [0.03427672, 0.03230119, 0.03591561, ..., 0.5335299 , 0.5332312 ,\n",
      "        0.5310561 ],\n",
      "       ...,\n",
      "       [0.7638639 , 0.77881366, 0.77567357, ..., 0.96902025, 0.96653014,\n",
      "        0.9724958 ],\n",
      "       [0.77948093, 0.7882018 , 0.7878615 , ..., 0.9684253 , 0.9758215 ,\n",
      "        0.9674425 ],\n",
      "       [0.7886159 , 0.7934579 , 0.7874305 , ..., 0.97681946, 0.9752751 ,\n",
      "        0.96789515]], shape=(256, 256), dtype=float32), 'processing_time': 7.435730218887329, 'point_count': 4096}\n",
      "{'image_path': 'public/dovenest.png', 'analysis': {'type': '3d_object', 'confidence': 0.6}, 'points': array([[-2.08101487, -2.08101487,  4.16202974],\n",
      "       [-1.97595871, -2.03969932,  4.07939863],\n",
      "       [-1.906045  , -2.03311467,  4.06622934],\n",
      "       ...,\n",
      "       [ 3.56964713,  3.81582969,  7.87784195],\n",
      "       [ 3.73155043,  3.85593544,  7.96064091],\n",
      "       [ 3.89975192,  3.89975192,  8.05110073]], shape=(4096, 3)), 'colors': array([[0.84705882, 0.83137255, 0.77254902],\n",
      "       [0.87058824, 0.85882353, 0.8       ],\n",
      "       [0.84705882, 0.83529412, 0.77254902],\n",
      "       ...,\n",
      "       [0.33333333, 0.3254902 , 0.29803922],\n",
      "       [0.35294118, 0.36078431, 0.35686275],\n",
      "       [0.25098039, 0.27058824, 0.27843137]], shape=(4096, 3)), 'depth_map': array([[0.41620296, 0.4082249 , 0.40988123, ..., 0.998357  , 0.9858955 ,\n",
      "        0.70261145],\n",
      "       [0.3911095 , 0.4068497 , 0.40837047, ..., 0.99900776, 0.9859887 ,\n",
      "        0.9884042 ],\n",
      "       [0.40706167, 0.40518937, 0.4045923 , ..., 0.9990775 , 0.99942094,\n",
      "        0.9983552 ],\n",
      "       ...,\n",
      "       [0.92814076, 0.93362606, 0.92939985, ..., 0.80732507, 0.8064613 ,\n",
      "        0.80875766],\n",
      "       [0.9395821 , 0.9389192 , 0.9353416 , ..., 0.80741054, 0.8119638 ,\n",
      "        0.80800354],\n",
      "       [0.9509357 , 0.9436531 , 0.9342766 , ..., 0.8130677 , 0.8135358 ,\n",
      "        0.8112331 ]], shape=(256, 256), dtype=float32), 'processing_time': 7.438642263412476, 'point_count': 4096}\n",
      "{'image_path': 'public/meme-snap.png', 'analysis': {'type': '3d_object', 'confidence': 0.6}, 'points': array([[-0.        , -0.        ,  0.        ],\n",
      "       [-0.01263889, -0.01304659,  0.02609319],\n",
      "       [-0.02981073, -0.03179812,  0.06359623],\n",
      "       ...,\n",
      "       [ 4.36996605,  4.67134301,  9.644063  ],\n",
      "       [ 4.50857744,  4.65886335,  9.61829853],\n",
      "       [ 4.6352441 ,  4.6352441 ,  9.56953621]], shape=(4096, 3)), 'colors': array([[0.74509804, 0.88627451, 0.94901961],\n",
      "       [0.74509804, 0.88627451, 0.94901961],\n",
      "       [0.74117647, 0.88627451, 0.95294118],\n",
      "       ...,\n",
      "       [0.28627451, 0.45882353, 0.29803922],\n",
      "       [0.28627451, 0.45882353, 0.29803922],\n",
      "       [0.28627451, 0.45882353, 0.29803922]], shape=(4096, 3)), 'depth_map': array([[0.        , 0.00144283, 0.00186736, ..., 0.20635958, 0.20361632,\n",
      "        0.1562495 ],\n",
      "       [0.00101416, 0.00124714, 0.00190694, ..., 0.20814952, 0.20482096,\n",
      "        0.20331687],\n",
      "       [0.00173679, 0.00214549, 0.00280026, ..., 0.2097662 , 0.20829366,\n",
      "        0.20491992],\n",
      "       ...,\n",
      "       [0.9652891 , 0.9671234 , 0.9665349 , ..., 0.9592151 , 0.95471585,\n",
      "        0.95073795],\n",
      "       [0.97063875, 0.9710748 , 0.9735524 , ..., 0.9597647 , 0.9586687 ,\n",
      "        0.9472994 ],\n",
      "       [0.9656326 , 0.9737032 , 0.97487956, ..., 0.96151567, 0.9558226 ,\n",
      "        0.94398296]], shape=(256, 256), dtype=float32), 'processing_time': 7.438768148422241, 'point_count': 4096}\n",
      "{'image_path': 'public/doveart1.png', 'analysis': {'type': '3d_object', 'confidence': 0.6}, 'points': array([[-0.27510676, -0.27510676,  0.55021352],\n",
      "       [-0.24261398, -0.25044024,  0.50088048],\n",
      "       [-0.23027517, -0.24562685,  0.4912537 ],\n",
      "       ...,\n",
      "       [ 3.56686138,  3.81285182,  7.87169409],\n",
      "       [ 3.68600138,  3.80886809,  7.8634696 ],\n",
      "       [ 3.79579553,  3.79579553,  7.83648109]], shape=(4096, 3)), 'colors': array([[0.83921569, 0.68627451, 0.50980392],\n",
      "       [0.79215686, 0.61960784, 0.45882353],\n",
      "       [0.81960784, 0.6627451 , 0.50196078],\n",
      "       ...,\n",
      "       [0.28627451, 0.27058824, 0.27058824],\n",
      "       [0.22352941, 0.20392157, 0.21960784],\n",
      "       [0.32156863, 0.30588235, 0.31764706]], shape=(4096, 3)), 'depth_map': array([[0.05502135, 0.05171755, 0.05063666, ..., 0.25894123, 0.25884345,\n",
      "        0.19540732],\n",
      "       [0.0500446 , 0.05100372, 0.05084216, ..., 0.26000667, 0.25848296,\n",
      "        0.26252478],\n",
      "       [0.05136713, 0.05169617, 0.05092014, ..., 0.26038563, 0.26152438,\n",
      "        0.26346436],\n",
      "       ...,\n",
      "       [0.78911567, 0.79125416, 0.7894916 , ..., 0.78240275, 0.7787336 ,\n",
      "        0.7796092 ],\n",
      "       [0.7951331 , 0.7948905 , 0.7949334 , ..., 0.78405106, 0.7838723 ,\n",
      "        0.7781088 ],\n",
      "       [0.79632527, 0.797917  , 0.7954322 , ..., 0.7871292 , 0.785125  ,\n",
      "        0.7829124 ]], shape=(256, 256), dtype=float32), 'processing_time': 7.439315319061279, 'point_count': 4096}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the directory where the images are stored\n",
    "image_directory = \"public\"  # Replace with your actual directory path\n",
    "\n",
    "# List of the image filenames\n",
    "image_filenames = [\n",
    "    \"numbers-stonks1.png\",\n",
    "    \"dovenest.png\",\n",
    "    \"meme-snap.png\",\n",
    "    \"doveart1.png\"\n",
    "]\n",
    "\n",
    "# Create the full paths to the images\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in image_filenames]\n",
    "\n",
    "class BatchFastDepthPipeline:\n",
    "    def __init__(self, model_type='midas_small', input_size=256, use_metal=True,\n",
    "                 max_batch_size=8, prefetch_buffer=16):\n",
    "        \"\"\"\n",
    "        Initialize the batch depth estimation pipeline\n",
    "\n",
    "        Args:\n",
    "            model_type: 'midas_small' or 'zoe_nano'\n",
    "            input_size: Input image size (256, 384, or 512)\n",
    "            use_metal: Use Apple Metal backend for acceleration\n",
    "            max_batch_size: Maximum images to process simultaneously\n",
    "            prefetch_buffer: Number of images to prefetch for processing\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.input_size = input_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.prefetch_buffer = prefetch_buffer\n",
    "        self.device = self._setup_device(use_metal)\n",
    "        self.model = self._load_model()\n",
    "\n",
    "        # Setup parallel processing\n",
    "        self.num_workers = min(8, mp.cpu_count())\n",
    "\n",
    "        print(f\"üöÄ Batch Pipeline initialized:\")\n",
    "        print(f\"   Model: {model_type}\")\n",
    "        print(f\"   Input size: {input_size}x{input_size}\")\n",
    "        print(f\"   Device: {self.device}\")\n",
    "        print(f\"   Max batch size: {max_batch_size}\")\n",
    "        print(f\"   Workers: {self.num_workers}\")\n",
    "\n",
    "    def _setup_device(self, use_metal):\n",
    "        \"\"\"Setup optimal device for M2 Mac\"\"\"\n",
    "        if use_metal and torch.backends.mps.is_available():\n",
    "            return torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            return torch.device('cuda')\n",
    "        else:\n",
    "            return torch.device('cpu')\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load lightweight depth estimation model\"\"\"\n",
    "        if self.model_type == 'midas_small':\n",
    "            model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            return model\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "\n",
    "    def preprocess_batch(self, image_paths: List[str]) -> Tuple[torch.Tensor, List[Tuple], List[float]]:\n",
    "        \"\"\"Preprocess multiple images in parallel\"\"\"\n",
    "        def process_single_image(image_path):\n",
    "            try:\n",
    "                img = cv2.imread(str(image_path))\n",
    "                if img is None:\n",
    "                    return None, None, None\n",
    "\n",
    "                # Convert BGR to RGB\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Store original dimensions\n",
    "                h, w = img.shape[:2]\n",
    "                scale = self.input_size / max(h, w)\n",
    "                new_h, new_w = int(h * scale), int(w * scale)\n",
    "                img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "                # Pad to square\n",
    "                pad_h = self.input_size - new_h\n",
    "                pad_w = self.input_size - new_w\n",
    "                img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "\n",
    "                # Convert to tensor\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "\n",
    "                return img, (h, w), scale\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "                return None, None, None\n",
    "\n",
    "        # Process images in parallel\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            results = list(executor.map(process_single_image, image_paths))\n",
    "\n",
    "        # Filter out failed images and stack tensors\n",
    "        valid_results = [(img, dims, scale) for img, dims, scale in results if img is not None]\n",
    "\n",
    "        if not valid_results:\n",
    "            return torch.empty(0), [], []\n",
    "\n",
    "        images = torch.stack([img for img, _, _ in valid_results])\n",
    "        dimensions = [dims for _, dims, _ in valid_results]\n",
    "        scales = [scale for _, _, scale in valid_results]\n",
    "\n",
    "        return images.to(self.device), dimensions, scales\n",
    "\n",
    "    def estimate_depth_batch(self, image_batch: torch.Tensor) -> List[np.ndarray]:\n",
    "        \"\"\"Estimate depth for a batch of images\"\"\"\n",
    "        if image_batch.size(0) == 0:\n",
    "            return []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            depth_batch = self.model(image_batch)\n",
    "            depth_maps = []\n",
    "\n",
    "            for i in range(depth_batch.size(0)):\n",
    "                depth = depth_batch[i].squeeze().cpu().numpy()\n",
    "                depth = (depth - depth.min()) / (depth.max() - depth.min())\n",
    "                depth_maps.append(depth)\n",
    "\n",
    "        return depth_maps\n",
    "\n",
    "    def create_sparse_point_cloud_batch(self, depth_maps: List[np.ndarray],\n",
    "                                      rgb_images: List[np.ndarray],\n",
    "                                      sample_rate: int = 4) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Create sparse point clouds for multiple images in parallel\"\"\"\n",
    "        def process_single_pointcloud(args):\n",
    "            depth_map, rgb_image = args\n",
    "            h, w = depth_map.shape\n",
    "\n",
    "            # Sample every Nth pixel for speed\n",
    "            y_coords, x_coords = np.meshgrid(\n",
    "                np.arange(0, h, sample_rate),\n",
    "                np.arange(0, w, sample_rate),\n",
    "                indexing='ij'\n",
    "            )\n",
    "\n",
    "            # Get depth and color values\n",
    "            depths = depth_map[y_coords, x_coords]\n",
    "            colors = rgb_image[y_coords, x_coords]\n",
    "\n",
    "            # Create 3D coordinates\n",
    "            focal_length = max(h, w)\n",
    "            cx, cy = w // 2, h // 2\n",
    "\n",
    "            z = depths * 10\n",
    "            x = (x_coords - cx) * z / focal_length\n",
    "            y = (y_coords - cy) * z / focal_length\n",
    "\n",
    "            points = np.stack([x.flatten(), y.flatten(), z.flatten()], axis=1)\n",
    "            colors = colors.reshape(-1, 3) / 255.0\n",
    "\n",
    "            # Remove invalid points\n",
    "            valid_mask = ~np.isnan(points).any(axis=1)\n",
    "            points = points[valid_mask]\n",
    "            colors = colors[valid_mask]\n",
    "\n",
    "            return points, colors\n",
    "\n",
    "        # Process point clouds in parallel\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            results = list(executor.map(process_single_pointcloud, zip(depth_maps, rgb_images)))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_3d_shape_batch(self, point_cloud_batch: List[Tuple[np.ndarray, np.ndarray]]) -> List[Dict]:\n",
    "        \"\"\"Analyze 3D shapes for multiple point clouds in parallel\"\"\"\n",
    "        def analyze_single_shape(args):\n",
    "            points, colors = args\n",
    "            if len(points) < 10:\n",
    "                return {\"type\": \"unknown\", \"confidence\": 0.0}\n",
    "\n",
    "            # Calculate basic statistics\n",
    "            bbox_min = np.min(points, axis=0)\n",
    "            bbox_max = np.max(points, axis=0)\n",
    "            bbox_size = bbox_max - bbox_min\n",
    "\n",
    "            z_values = points[:, 2]\n",
    "            z_std = np.std(z_values)\n",
    "\n",
    "            aspect_ratio_xy = bbox_size[0] / bbox_size[1] if bbox_size[1] > 0 else 1.0\n",
    "            aspect_ratio_z = bbox_size[2] / max(bbox_size[0], bbox_size[1]) if max(bbox_size[0], bbox_size[1]) > 0 else 1.0\n",
    "\n",
    "            # Simple classification\n",
    "            if z_std < 0.5 and aspect_ratio_z < 0.1:\n",
    "                return {\"type\": \"flat_surface\", \"confidence\": 0.8}\n",
    "            elif aspect_ratio_xy > 3.0 or aspect_ratio_xy < 0.3:\n",
    "                return {\"type\": \"elongated_object\", \"confidence\": 0.7}\n",
    "            elif z_std > 1.0:\n",
    "                return {\"type\": \"3d_object\", \"confidence\": 0.6}\n",
    "            else:\n",
    "                return {\"type\": \"unknown\", \"confidence\": 0.3}\n",
    "\n",
    "        # Analyze shapes in parallel\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            results = list(executor.map(analyze_single_shape, point_cloud_batch))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def process_batch(self, image_paths: List[str], sample_rate: int = 4,\n",
    "                     save_results: bool = False, output_dir: str = \"batch_results\") -> List[Dict]:\n",
    "        \"\"\"Process a batch of images efficiently\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create output directory if saving results\n",
    "        if save_results:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Split into manageable batches\n",
    "        all_results = []\n",
    "\n",
    "        for i in range(0, len(image_paths), self.max_batch_size):\n",
    "            batch_paths = image_paths[i:i + self.max_batch_size]\n",
    "            batch_start = time.time()\n",
    "\n",
    "            print(f\"üì¶ Processing batch {i//self.max_batch_size + 1}/{(len(image_paths) + self.max_batch_size - 1)//self.max_batch_size}\")\n",
    "\n",
    "            # Preprocess batch\n",
    "            image_batch, dimensions, scales = self.preprocess_batch(batch_paths)\n",
    "\n",
    "            if image_batch.size(0) == 0:\n",
    "                print(\"‚ö†Ô∏è  No valid images in batch\")\n",
    "                continue\n",
    "\n",
    "            # Estimate depth for batch\n",
    "            depth_maps = self.estimate_depth_batch(image_batch)\n",
    "\n",
    "            # Load RGB images for point cloud generation\n",
    "            rgb_images = []\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    img = cv2.imread(str(path))\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (self.input_size, self.input_size))\n",
    "                    rgb_images.append(img)\n",
    "                except:\n",
    "                    rgb_images.append(np.zeros((self.input_size, self.input_size, 3), dtype=np.uint8))\n",
    "\n",
    "            # Create point clouds\n",
    "            point_clouds = self.create_sparse_point_cloud_batch(depth_maps, rgb_images, sample_rate)\n",
    "\n",
    "            # Analyze shapes\n",
    "            analyses = self.analyze_3d_shape_batch(point_clouds)\n",
    "\n",
    "            # Compile results\n",
    "            for j, (path, analysis, (points, colors), depth_map) in enumerate(zip(batch_paths, analyses, point_clouds, depth_maps)):\n",
    "                result = {\n",
    "                    'image_path': str(path),\n",
    "                    'analysis': analysis,\n",
    "                    'points': points,\n",
    "                    'colors': colors,\n",
    "                    'depth_map': depth_map,\n",
    "                    'processing_time': time.time() - batch_start,\n",
    "                    'point_count': len(points)\n",
    "                }\n",
    "\n",
    "                # Save individual results if requested\n",
    "                if save_results:\n",
    "                    result_path = os.path.join(output_dir, f\"{Path(path).stem}_result.json\")\n",
    "                    self._save_result(result, result_path)\n",
    "\n",
    "                all_results.append(result)\n",
    "\n",
    "            batch_time = time.time() - batch_start\n",
    "            print(f\"   ‚úÖ Batch completed in {batch_time:.2f}s ({len(batch_paths)} images)\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nüéØ Batch processing complete:\")\n",
    "        print(f\"   Total images: {len(image_paths)}\")\n",
    "        print(f\"   Successfully processed: {len(all_results)}\")\n",
    "        print(f\"   Total time: {total_time:.2f}s\")\n",
    "        print(f\"   Average per image: {total_time/len(all_results):.2f}s\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def _save_result(self, result: Dict, output_path: str):\n",
    "        \"\"\"Save result to JSON file (without numpy arrays)\"\"\"\n",
    "        # Create a serializable version\n",
    "        serializable_result = {\n",
    "            'image_path': result['image_path'],\n",
    "            'analysis': result['analysis'],\n",
    "            'processing_time': result['processing_time'],\n",
    "            'point_count': result['point_count'],\n",
    "            'depth_map_shape': result['depth_map'].shape,\n",
    "            'points_shape': result['points'].shape if len(result['points']) > 0 else (0, 0)\n",
    "        }\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(serializable_result, f, indent=2)\n",
    "\n",
    "    def process_directory(self, input_dir: str, extensions: List[str] = None,\n",
    "                         sample_rate: int = 4, save_results: bool = False,\n",
    "                         output_dir: str = \"batch_results\") -> List[Dict]:\n",
    "        \"\"\"Process all images in a directory\"\"\"\n",
    "        if extensions is None:\n",
    "            extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "\n",
    "        # Find all image files\n",
    "        image_paths = []\n",
    "        for ext in extensions:\n",
    "            image_paths.extend(Path(input_dir).glob(f\"*{ext}\"))\n",
    "            image_paths.extend(Path(input_dir).glob(f\"*{ext.upper()}\"))\n",
    "\n",
    "        print(f\"üìÅ Found {len(image_paths)} images in {input_dir}\")\n",
    "\n",
    "        if not image_paths:\n",
    "            print(\"‚ö†Ô∏è  No images found\")\n",
    "            return []\n",
    "\n",
    "        return self.process_batch(image_paths, sample_rate, save_results, output_dir)\n",
    "\n",
    "    def get_batch_summary(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate summary statistics for batch processing\"\"\"\n",
    "        if not results:\n",
    "            return {}\n",
    "\n",
    "        # Aggregate statistics\n",
    "        shape_types = {}\n",
    "        total_points = 0\n",
    "        processing_times = []\n",
    "\n",
    "        for result in results:\n",
    "            shape_type = result['analysis']['type']\n",
    "            shape_types[shape_type] = shape_types.get(shape_type, 0) + 1\n",
    "            total_points += result['point_count']\n",
    "            processing_times.append(result['processing_time'])\n",
    "\n",
    "        return {\n",
    "            'total_images': len(results),\n",
    "            'shape_distribution': shape_types,\n",
    "            'total_points_generated': total_points,\n",
    "            'average_points_per_image': total_points / len(results),\n",
    "            'average_processing_time': np.mean(processing_times),\n",
    "            'total_processing_time': sum(processing_times)\n",
    "        }\n",
    "\n",
    "# Utility functions for batch processing\n",
    "def quick_batch_test(num_images: int = 5):\n",
    "    \"\"\"Quick test with multiple synthetic images\"\"\"\n",
    "    test_dir = \"test_batch\"\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Create test images\n",
    "    for i in range(num_images):\n",
    "        test_img = np.random.randint(0, 255, (400, 400, 3), dtype=np.uint8)\n",
    "        # Add different shapes\n",
    "        if i % 3 == 0:\n",
    "            cv2.circle(test_img, (200, 200), 80, (255, 0, 0), -1)\n",
    "        elif i % 3 == 1:\n",
    "            cv2.rectangle(test_img, (50, 50), (350, 150), (0, 255, 0), -1)\n",
    "        else:\n",
    "            cv2.ellipse(test_img, (200, 200), (100, 50), 45, 0, 360, (0, 0, 255), -1)\n",
    "\n",
    "        cv2.imwrite(f\"{test_dir}/test_{i:03d}.jpg\", test_img)\n",
    "\n",
    "    # Run batch pipeline\n",
    "    pipeline = BatchFastDepthPipeline(input_size=256, max_batch_size=4)\n",
    "    results = pipeline.process_directory(test_dir, sample_rate=8, save_results=True)\n",
    "\n",
    "    # Get summary\n",
    "    summary = pipeline.get_batch_summary(results)\n",
    "\n",
    "    # Cleanup\n",
    "    import shutil\n",
    "    shutil.rmtree(test_dir)\n",
    "\n",
    "    print(\"\\nüìä Batch Test Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "    return results, summary\n",
    "\n",
    "# Memory-efficient streaming processor for very large batches\n",
    "class StreamingDepthProcessor:\n",
    "    def __init__(self, pipeline: BatchFastDepthPipeline, chunk_size: int = 100):\n",
    "        self.pipeline = pipeline\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def process_large_dataset(self, image_paths: List[str], output_file: str = \"streaming_results.jsonl\"):\n",
    "        \"\"\"Process very large datasets with streaming output\"\"\"\n",
    "        total_images = len(image_paths)\n",
    "        processed = 0\n",
    "\n",
    "        with open(output_file, 'w') as f:\n",
    "            for i in range(0, total_images, self.chunk_size):\n",
    "                chunk_paths = image_paths[i:i + self.chunk_size]\n",
    "                results = self.pipeline.process_batch(chunk_paths, save_results=False)\n",
    "\n",
    "                # Write results incrementally\n",
    "                for result in results:\n",
    "                    # Convert numpy arrays to lists for JSON serialization\n",
    "                    json_result = {\n",
    "                        'image_path': result['image_path'],\n",
    "                        'analysis': result['analysis'],\n",
    "                        'processing_time': result['processing_time'],\n",
    "                        'point_count': result['point_count']\n",
    "                    }\n",
    "                    f.write(json.dumps(json_result) + '\\n')\n",
    "                    f.flush()\n",
    "\n",
    "                processed += len(results)\n",
    "                print(f\"üíæ Streamed {processed}/{total_images} results\")\n",
    "\n",
    "        print(f\"‚úÖ Streaming complete. Results saved to {output_file}\")\n",
    "\n",
    "print(\"üéØ Batch-optimized setup complete!\")\n",
    "print(\"üìù Run 'quick_batch_test()' to test batch processing.\")\n",
    "print(\"üìÅ Use 'process_directory()' for real datasets.\")\n",
    "print(\"üíæ Use 'StreamingDepthProcessor' for very large datasets.\")\n",
    "\n",
    "# Initialize the BatchFastDepthPipeline\n",
    "pipeline = BatchFastDepthPipeline(input_size=256, max_batch_size=4)\n",
    "\n",
    "# Process the batch of images\n",
    "results = pipeline.process_batch(image_paths, save_results=True)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-viz1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
